---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---

# 预训练

预训练指对神经网络模型在大规模文本数据上进行训练，这通常基于无监督学习，需要大量的训练数据，为微调提供基础。详细的内容可以参考[知乎文章](https://zhuanlan.zhihu.com/p/144927093)。

## 子词

在介绍预训练之前，课程先介绍了子词，这在以往的课程中作为一个单独的章节，可以参考[中文资料](https://juejin.cn/post/7098237869516390437)，但在较新的课程中仅作为一个介绍。其基本的理念就是通过统计的方法从海量文本中学习，建立一个新的词库，这可以解决很多的诸如稀疏性和过大计算量的问题，通常的做法是从字符开始进行构建，具体的算法有很多，更为具体的可以参考[知乎文章](https://zhuanlan.zhihu.com/p/90151246)。

## 编码器预训练

这里主要介绍 `Bert` 模型，这部分主要参考了[知乎文章](https://zhuanlan.zhihu.com/p/403495863)。

### 模型结构

`Bert` 模型主要由 Transformer 编码器组成，首先对输入的文本进行嵌入，包括 Token 嵌入、位置嵌入和片段嵌入，然后经过多个 Transformer 编码器层，最后进行输出。

### 训练任务

`Bert` 模型主要有两个预训练任务，分别是 Masked Language Model 和 Next Sentence Prediction，这两个都是自监督任务，训练数据不需要包含标签。

Masked Language Model 即掩码语言模型，对输入文本进行随机掩码，预测掩码的词，这类似于完形填空。 `Bert` 模型训练的时候一般将文本的 15% 进行掩码，同时并不是都替换为掩码字符或词语，而是有 80% 的概率替换为掩码字符，10% 的概率替换为随机字符，10% 的概率不替换。

Next Sentence Prediction 即下一句话预测，预测两个句子是否是连续的，训练数据从平行语料中随机抽取连续的两句话，并有 50% 的概率保留抽取的两句话，50% 的概率第二句话是随机抽取的。后续的研究发现，这个任务的训练或许不是必要的。

更为具体的内容参考[知乎文章](https://zhuanlan.zhihu.com/p/403495863)。

## 编码器-解码器预训练

这里主要介绍 T5 模型，这部分内容主要参考[知乎文章](https://zhuanlan.zhihu.com/p/88438851)和[知乎文章](https://zhuanlan.zhihu.com/p/589869911)。

T5 模型主要有 Transformer 编码器和解码器组成，编码器接受文本并输出上下文表示，解码器接受上下文表示和目标文本并生成文本。预训练的数据主要采用跨度损坏的方法，即对文本选择一个跨度，可以为一个单词或多个单词，将其替换为一个特殊的字符，作为编码器的输入，而解码器则需要尝试还原被替换的跨度。

更具体的参考[知乎文章](https://zhuanlan.zhihu.com/p/88438851)和[知乎文章](https://zhuanlan.zhihu.com/p/589869911)，具体的实现可以参考第四次作业。

## 解码器预训练

解码器预训练与上面提到的编码器预训练和编码器-解码器预训练类似，区别在于模型主要由解码器组成，成功的案例为 GPT-2 ，更为具体的内容可以参考[课件](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/slides/cs224n-spr2024-lecture09-pretraining-updated.pdf)等资料。
