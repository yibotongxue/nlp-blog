---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---

# 人类反馈强化学习及其改进

本文介绍人类反馈强化学习及其改进的相关内容。

## 强化学习精要

在介绍人类反馈强化学习之前，我们简单地介绍强化学习的内容，我们不想过多的涉及强化学习的深刻理论等，只是提供一个介绍人类反馈强化学习必须的基础讲解，更深刻的内容可以看[这个网站](https://datawhalechina.github.io/easy-rl/#/)。

强化学习研究的主要是马尔科夫过程的问题，其核心性质在于未来仅与当前有关，而无关于历史，也就是说，无论智能体是如何达到当前的状态的，都不对未来有任何影响，唯一能影响未来发展的是智能体当前的状态。强化学习引入了许多的概念，我们不做严格的阐释，而做出形象的理解。

智能体的状态即是当前智能体的状态（有点废话）。动作即智能体能采取的动作，将导致其状态的改变。策略即智能体基于状态采取动作的策略，实际上可以用智能体在一定状态下采取特定动作的条件概率来表示。奖励即智能体在一个状态下采取一个动作得到的环境的奖励。在大部分情况下，奖励只与当前的状态有关。累积奖励即从当前开始的奖励的累积，但需要考虑未来折扣，即越靠后的奖励越不稳定，需要乘以一个系数 $\gama$ 。累积奖励的求取通常是基于一个特定的策略的，因为我们需要知道状态的变化。我们定义价值函数为给定初始状态和策略下累积奖励的期望，这里强调所谓的期望是因为我们的策略是一个概率描述，状态转移也是一个概率事件。事实上，我们很容易注意到，价值函数中除了即时的当前状态的奖励，剩下的项实际上等于折扣因子乘以下一个状态的价值函数，并考虑概率，用期望表示，这就是贝尔曼方程。基于这些，我们定义最优价值函数为所有策略中最高的价值函数，定义最优策略为取得最高价值的策略，并很容易导出最有价值函数的贝尔曼方程。我们还可以定义状态动作价值函数，即当前的奖励加上动作带来的未来的状态的奖励期望的和。

> 这些概念确实很多，其核心就是在描述智能体采取动作、变更状态、获得奖励的过程，我们不必理会许多严谨但复杂的公式，而需要理解其实质。

最优策略和最优价值函数的取得有两种方法，一种是直接求最优价值函数，从而得出最优策略，即价值迭代。价值迭代即初始化所有状态的价值为 $0$ ，每次迭代每个状态的所有可能动作，用当前的价值函数估计每个动作得到的下一个状态的价值期望，取最高者加上当前状态的奖励，得到新的价值函数。价值迭代实际上每次都采取了一个贪心的策略，每个状态采取当前价值函数的最优动作。一种是直接求最优策略，从而得出最优价值函数，即策略迭代。策略迭代即从完全随机的策略开始迭代，每次根据当前策略计算价值函数，再基于当前价值函数采用类似价值迭代的贪心策略确定新的策略，循环往复。

事实上在更新价值函数的时候我们可以不要那么激进地直接更新为新的价值函数，而是可以加上两者差值乘以一个步长，以增加稳定性。

> 这里的内容也不是很好理解，但实际上只需要知道我们通过贪心的方法从价值函数得到策略，又根据策略计算价值函数即可。

很多问题下，状态可能很多，奖励也不定然是可以直接获得，可能需要智能体在探索中获取，于是有了两种学习方法——时序差分和蒙特卡洛。名字听起来都很吓人，但实际上表达的意思很简单。考虑智能体在环境中探索了一段时间，记录了一系列的状态、动作、奖励等，蒙特卡洛即探索结束后根据片段的奖励计算探索观测到的累积奖励更新原来的价值函数（回忆一下，价值函数就是累积奖励的期望！），而时序差分则动态地在线更新，每有一次动作则根据当前的奖励和下一个状态的价值函数更新价值函数。很显然蒙特卡洛方差无偏但方差更大，而时序差分则相反。作为折中，我们可以使用多步时序差分，即根据有限长的片段更新价值函数。更进一步，我们可以将不同的步长加权平均，即 $TD(\lambda)$ 方法。

> 这里的内容可能看得云里雾里的，不知道这些到底是在干什么。事实上，考虑RLHF中的实际问题，RLHF中我们需要有一个网络估计价值函数或者说优势，而这里的方法就可以作为这个网络的训练方法。换句话说，我们就是给出了一个训练评价者网络的方法。

我们进一步定义优势函数为动作价值函数减去价值函数，实际上就是动作价值函数减去动作价值函数的期望。定义广义优势函数为多步时序差分中每个步长的更新项的加权和。用优势函数取代价值函数可以降低方差。

> 优势函数通常是决定动作的选择，在RLHF中常用来训练演员网络。

## 策略梯度优化和PPO

上面我们简单介绍了强化学习的基本内容（事实上可读性很差，如果没有了解过的话很难看懂上面的部分），我们接下来介绍策略梯度优化的问题。上面我们介绍的都是强化学习的理论，我们可以隐约感觉到主要是针对工程控制的，我们需要过渡到深度学习领域的强化学习，从而才能理解人类反馈强化学习。

我们可以将几个部分改为神经网络建模，一个是策略的取得，我们从迭代求取最优策略改为优化策略网络，所谓策略网络，可以认为它会给一个状态输出一系列动作的分数或概率或对数概率，也可以认为它会根据一个状态和动作对打分。一个是奖励的取得，很多情况下我们需要对奖励建模。一个是价值函数或优势函数的取得，我们需要一个神经网络取得一个状态的价值或优势。其中最核心的是策略网络，它是我们动作的决策者，是优化的最终目标。

策略网络的优化关键是策略梯度的求解，我们有策略梯度定理：

:::info 策略梯度定理
对任意可微的策略 $\pi_{\theta}(a|s)$ ，任意策略的目标函数 $J=J_1, J_{avR}, J_{avV}$ ，其策略梯度是

$$
\frac{\partial J(\theta)}{\partial \theta} = E_{\pi_{\theta}}[\frac{\partial log\pi_{\theta}(a|s)}{\partial \theta}Q^{\pi_{\theta}}(s,a)]
$$
:::

这里提到的 $J_1, J_{avR}, J_{avV}$ 分别是起始目标函数、平均奖励目标函数和平均价值函数，而 $Q^{\pi_{\theta}}(s,a)$ 就是对应的要被平均的部分（比如奖励、价值）。公式的数学推导非常繁琐，有很多技巧，我不想过多展开，通俗的理解就是奖励或价值或其他的基于状态和动作的信号的期望，亦即我们的目标函数，其梯度等于对数概率的梯度与信号乘积的期望。这里的信号通常需要有另一个网络取得，即上面提到的评论家网络。

> 事实上我们不必深究其中的数学问题，只需要知道我们是有办法可以训练策略网络的即可。

取得了策略梯度，我们自然而然地可以藉由梯度上升最大化目标函数来优化策略网络。同时我们需要训练评论家网络。上面的 $Q^{\pi_{\theta}}(s,a)$ 通常可以替代为优势函数，以减少方差，这对于网络结构几乎没有改变，因为 $Q^{\pi_{\theta}}(s,a)$ 是通过一个单独的评论家网络取得的。

实际实验发现，自然梯度法并不总是成功的，梯度上升的步长不好掌控，很容易出现训练的不稳定。研究人员提出了TRPO以解决这个问题，即约束更新后的策略与原来的策略的KL散度，在这个约束情况下最大化目标函数。实际上我们可以通过加入惩罚项实现这个约束。PPO在TRPO的基础上进一步限制了重要性采样中的重要率。这里提到的重要性采样涉及到TRPO的另一个改进，TRPO使用重要性采样，在原来的策略上采样的样本中，而加上新策略的重要性求得新策略的期望，得到新策略的样本，这减少了采样次数。注意这里我们没有引入评论家网络，因此我们的优势函数是广义优势函数，需要通过采样计算得到。

## 人类反馈强化学习

我们可以开始介绍人类反馈强化学习的基本结构。基本的流程是，我们先对预训练模型进行监督微调（一些RLHF方法省略了这个步骤），然后进行奖励建模（即训练一个奖励模型），然后使用PPO算法进行强化学习。对应强化学习的概念，奖励信号由奖励模型取得，当前的大语言模型即为策略模型或者演员模型，优势通常由一个单独的网络（称为评论家模型或价值模型）取得，旧的策略即我们的预训练模型或监督微调模型得到的。

### 奖励建模

奖励模型通常从偏好对中训练，所谓偏好对，即大语言模型对一个问题的两个回答，标注有人类偏好（即标注哪一个更好），其核心在于将模型认为更好的回答的概率优于更差的回答的概率或对数概率最大化，这个概率通过两个回答的奖励的差的 sigmoid 表示，亦即用指数的占比表示，而以负对数概率为损失函数。所谓的奖励模型，实际上就是一个预训练模型经过一个线性层，必要时可能再经过一个正规化。对于一个标记序列，我们通常取最后一个标记位上的奖励作为序列的奖励；一些情况下，我们也需要计算标记级别的偏好概率，这时我们需要找出两个回答出现分歧的地方，对往后逐个标记的奖励计算偏好概率，再求平均。

### 强化学习

训练好奖励模型后，我们可以使用奖励模型得到的奖励作为奖励信号，进行强化学习。首先要进行探索，即从数据集中采用提示词，使用当前的语言模型（即策略模型）得到响应，使用奖励模型得到响应的奖励，使用价值函数计算价值，再用奖励减去价值得到优势，然后我们可以来更新两个模型。

演员网络或策略网络，我们可以以上面提到的优势期望的负数为损失函数，加上KL散度的惩罚项，其中优势期望使用重要性采样取得，并对重要性裁剪，和未裁剪的取较小者。评论者网络或价值模型，其损失函数通常为我们通过奖励信号和既往的价值函数计算的得到的广义优势函数的折扣回报与当前的价值函数的均方误差，并可能需要对新的价值函数偏离旧的价值函数的幅度加以裁剪。

## 人类反馈强化学习的改进

人类反馈强化学习（RLHF）存在诸多的问题，比如需要训练多个模型，训练十分困难等。为此，研究者提出了许多改进的方法，这里介绍RRHF、PRO和DPO。

### RRHF

RRHF使用排序损失和交叉熵损失直接训练策略模型。模型对有排序标注的一系列响应分别计算条件对数概率，计算公式为

$$
p_i = \frac{\sum_tlogP_{\pi}(y_{i,t}|x,y_{i,\lt t})}{||y_i||}
$$

实际上就是每个标记的条件概率的乘积的对数，并对于长度归一化。然后计算排名损失，公式为

$$
L_{rank} = \sum\limits_{r_i\lt r_j}max(0, p_i - p_j)
$$

亦即逆序的条件对数概率的差的和。同时我们需要加上监督微调的交叉熵损失

$$
L_{sft} = -\sum\limits_{t}logP_{\pi}(y_{i',t}|x, y_{i', \lt t})
$$

总损失为两个损失的无权和。

### PRO

PRO与RRHF十分类似，也是基于排序的对齐方法，不同之处在于RRHF考虑的是一对响应之间的排序问题，而PRO考虑整个序列的排序问题。定义损失函数为

$$
L = -log\prod\limits_{k=1}^{n-1}\frac{exp(r_{\pi}(x,y^k))}{\sum_{i=k}^nexp(r_{\pi}(x,y^i))}
$$

即每个响应与排序在其后的响应的Softmax概率的乘积的负对数。

### DPO

DPO又称直接偏好对齐，其核心在于使用策略模型直接得出偏好概率，然后最大化偏好概率。DPO的偏好概率的推导较为复杂，其核心在于通过策略模型与奖励模型的关系，从用奖励指数和的占比定义的概率推导为直接用策略模型得到的概率表示的概率，其计算公式为

$$
p(y_1\gt y_2|x) = \sigma (\beta log\frac{\pi(y_1|x)}{\pi_{ref}(y_1|x)} - \beta log\frac{\pi(y_2|x)}{\pi_{ref}(y_2|x)})
$$

而最后，我们优化的损失函数就是上面概率的负对数的期望。其中涉及的推导相对复杂，我们不详细展开。

## 安全对齐

以上我们讲的多是偏好对齐，对于安全对齐，我们还需要引入代价函数，优化目标是在代价函数不超过一定范围内的最大化奖励期望。代价函数与奖励函数很类似，不同之处在于其训练是还需要考虑符号损失，这是因为安全不仅是安全程度的问题，还有是否安全的问题，与一般的人类偏好不同。符号损失一般定义为代价符号与代价乘积的sigmoid的对数，总的损失为安全偏好损失、安全符号损失和不安全符号损失的无权和或平均。安全内容的安全符号为+1，代价符号为-1，不安全内容则相反。
