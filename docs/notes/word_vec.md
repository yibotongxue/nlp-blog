---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---
# 词向量

这里为[cs224n](https://web.stanford.edu/class/cs224n/)第一讲的笔记，参考的中文资料为[这篇知乎文章](https://zhuanlan.zhihu.com/p/527211805)。

## WordNet

计算机处理文本词汇，一种方式就是 WordNet ，这是一个大型的英语词汇数据库，单词之间主要为同义关系，这些同义的单词会被分组为无序的同义词集合，而对于不同的同义词集合，相同词性的又有上位词和下位词的关系（表示泛化与具体）、整体与部分的关系、动词关系、形容词关系、副词关系，WordNet世纪由实际上由四个子图组成，跨词性的关系很少，大部分都是词法关系，还有一些关系表示名词是动词代表动作的某个语义角色。

具体的可以参考[这篇知乎文章](https://zhuanlan.zhihu.com/p/366370332)。

WordNet有许多弊端，是由少数专家创造的，本身带有主观性，而且不能精确的表示词的关系，以及增删词汇都不方便。

## 基于上下文的词汇表征

在传统的自然语言处理中，通常需要对文本进行离散表征，常见的方法是用独热向量，即只有一个维度为 $1$，其他维度为 $0$ 的向量，但这样有明显的弊端，向量的维度太高，每个向量都是正交的，无法表示词之间的相似关系。对于词之间的相似关系，一种解决方法是使用 `WordNet` ，另一种方法则是通过对大量数据的学习词向量本身的关系，从而建立稠密的、能表示词语关系的表征，其中比较有效的就是**基于上下文的词汇表征**。

所谓上下文，指的是一个词语在文本中出现时其附近的词语，通常由一个固定的窗口表示，而基于上下文的词汇表示就是通过海量数据中词语的上下文描述这个词语。这个很抽象，简单的理解可以是我们需要通过数据的学习求出一种表征方式，而这个基于上下文。

## Word2Vec

Word2Vec是一种通过迭代优化求得基于上下文的词汇表征的具体方法。其基本的思路就是通过随机初始化所有的词汇向量，而后通过最大化每一个词汇的上下文出现的似然概率求解词汇向量，这事实上是一个优化问题，优化的函数一般就是似然概率的负对数，优化方法一般是梯度下降。Word2Vec有两种变体，skip-gram通过中心词汇预测上下文，而CBOW通过上下文预测中心词汇。

### 似然函数和优化函数

对于每一个词汇，以其为中心建立一个固定大小的窗口，这个窗口中的每一个词汇即为其上下文，似然概率显然为

$$
P(o|c)
$$

其中 $o$ 表示上下文词汇，而 $c$ 表示中心词汇，而总的似然概率即为所有窗口所有上下文词汇的这个概率的乘积，即

$$
\prod_{j=1}^{T} \prod_{-m\le k\le m, k \ne 0}P(w_{j+k}|w_j)
$$

而优化的目标函数即为

$$
-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m\le k\le m, k \ne 0} log(P(w_{j+k}|w_j))
$$

这里多加了个平均，事实上在优化的问题没有区别，但可能可以使得具体调整参数的时候更为方便，学习率不需要依赖于数据量的大小（这个是我个人的理解，没有出处，慎重参考）。注意这里多了个负号，这个是必须的，因为我们需要的是使目标函数最小。

这个目标函数又成为代价函数或损失函数。

### 概率计算

概率 $P(o|c)$ 的计算通常使用 softmax 概率，我们需要先定义其相似程度，而后将相似程度取指数归一化，即可得到概率。相似程度通常通过词向量的点乘表示，因而概率即可表示为

$$
P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}
$$

其中 $v_c$ 表示中心词汇的词向量， $u_o$ 表示上下文词汇的次向量，而 $w$ 表示词汇空间所有词汇的任意一个词汇， $u_w$ 表示其词向量。

:::details 为什么使用点积
关于为什么使用点乘表示相似程度，我没有找到比较让人信服的答案。一个比较通常出现的说法是计算了余弦相似度，认为应该建立在都为单位向量的前提下（参考[知乎问题](https://www.zhihu.com/question/268273151)），这个解释是比较符合我们的直觉的，但事实上这个问题似乎不能保证所有向量的模长不会改变，对于此一个比较好的解释我们通常会进行l2归一化（参考[知乎文章](https://zhuanlan.zhihu.com/p/159244903)），但似乎不是所有的方法都有进行归一化。另一个解释是使用点乘要求了向量之间不仅在方向上相近，在量级上也相近，参考这个[知乎回答](https://www.zhihu.com/question/613803751)，但这个似乎不能理解。
我个人的理解是，可能只是大家发现直接使用点积相似度和使用余弦相似度没有太大的效果上的差异，而点积通常更快，所以就如此了。
:::

### 梯度计算

我们通常使用梯度下降的方法求解优化问题，这里我们需要优化的参数是所有的词向量。观察我们上面的式子，很容易发现损失函数实际上就是交叉熵损失，这个的梯度很好求解，你可以具体去推一下，[文章](https://zhuanlan.zhihu.com/p/527211805)中有较完整的推导了，但只是对于中间词汇的，对于上下文词汇也需要推导。

### 优化调整

对于 `Word2Vec` 有若干优化调整。

#### Mini-batch

上面的推导中，我们默认将文本中所有位置的词汇都计算了损失，如果文本数据很大，则需要很多的计算资源，作为优化，我们可以使用随机梯度下降，每次只选择一个样本进行训练，但这样参数波动较大，一般会将一个小的批量的样本一起训练，也就是Mini-batch。

#### 只更新出现的向量

词汇空间往往很大，如果使用 Mini-batch ，覆盖的词汇量一般很小，这意味着我们计算出来的梯度是稀疏的，因为只有出现在这批数据中的词汇对应的梯度才不为 $0$ ，作为优化，我们可以通过稀疏矩阵操作只更新出现的向量，或者可以记录每个单词的哈希或者散列。这样有个好处就是在分布式训练的过程中，我们不需要传输很大规模的数据，只需要传输出现的向量的梯度数据。

#### 使用负例采样

这部分同时也参考了[这篇知乎文章](https://zhuanlan.zhihu.com/p/153502072)。

训练过程中计算 Softmax 的开销是很大的，特别是当文本数据量很大的时候，遍历整个文本归一化的分母是并不容易的，对此的一种改进方法是负例采样。负例采样简单来说就是对于中心次的上下文词汇，其损失函数的计算不是在整个文本中进行，而是选择若干个噪声词，标记为负例，而上下文词汇为正例，要最大化的概率定义为上下文词汇与中心词汇同时出现且噪声词与中心词同时出现的概率，损失函数则为这个概率的负对数。

具体的细节，词与中心词同时出现的概率我们使用sigmoid计算，即对于中心词汇 $w_c$ 和词汇 $w_o$ ，其向量为 $v_c$ 和 $u_o$ ，则同时出现的概率为

$$
P(w_o|w_c) = \frac{1}{1+exp(-u_o^Tv_c)}
$$

噪声词汇的选择根据词汇在文本中出现的频率随机选择，简单的方法是其出现的频率即为被选为噪声词汇的概率，但一般的方法是以频率的 $0.75$ 次方为其概率，这样可以使得出现频率较低的词汇也有较大的可能被选中。

## 基于共现矩阵的词向量构建

Word2Vec通过概率预测的方式构建每个词的向量，即基于预估模型，除此之外，我们可以基于计数，通过对全局统计数据来直接估计。

### 共现矩阵

表示上下文一个常用的方法就是共现矩阵，矩阵为一个方阵，行数及列数为词空间词汇数目，每一个元素表示两个词汇同时出现在一个窗口的次数，具体地说，我们通过一个窗口滑动遍历整个文本，这个窗口内同时出现的元素对应的行和列索引到的元素加一。很容易发现这样得到的矩阵是对称阵，而每一行或每一列即为词向量。

### SVD压缩

这部分内容也参考了[这篇知乎文章](https://zhuanlan.zhihu.com/p/147312345)。

通过简单的计数得到的共现矩阵是十分稀疏的，每一个向量维度很高，需要进行降维。一个常见的降维方法就是SVD。简单的说就是对共现矩阵进行SVD分解，取中间矩阵的前 $k$ 个对角线元素组成的对角矩阵 ，其中 $k$ 可以由百分比确定，对于两个正交矩阵亦取对应的行和列，课程中的示例是取了第一个正交矩阵的前 $k$ 列，而取第二个矩阵的前 $k$ 行，这样得到的新矩阵每一行则为一个 $k$ 维向量，即为压缩后的词向量。

SVD压缩的一个重要问题就是计算开销很大。

## GloVe 模型

这部分内容也参考了[这篇博客](https://blog.csdn.net/u014665013/article/details/79642083)、[这篇知乎文章](https://zhuanlan.zhihu.com/p/147312345)和[这篇文章](https://www.showmeai.tech/article-detail/232)。

先回顾共现矩阵的定义，并介绍一些统计性质。记共现矩阵为 $X$ ，而 $X_{ij}$ 为其第 $i$ 行第 $j$ 列的元素，单词 $j$ 出现在单词 $i$ 上下文（或者说共现）的次数，而 $X_i = \sum\limits_{k=1}^{T}X_{ik}$ 。我们很容易得到单词 $j$ 出现在单词 $i$ 上下文的概率可以写作 $\frac{X_{ij}}{X_i}$ 。再来回顾迭代优化的 Word2Vec 算法，在那里我们定义了共现或者出现在上下文的概率为向量点乘结果的 softmax 。

GloVe模型的推导有两种方法，一种是通过概率比率，一种是基于 skip-gram 算法的改进，这里一并介绍。列举的参考文章中，[这篇博客](https://blog.csdn.net/u014665013/article/details/79642083)是基于概率比率，而[这篇知乎文章](https://zhuanlan.zhihu.com/p/147312345)和[这篇文章](https://www.showmeai.tech/article-detail/232)是基于 skip-gram 算法的改进。

### 原论文的推导

具体的可以参考[这里](https://aclanthology.org/D14-1162/)。

### 基于概率比率的推导

记概率比率

$$
ratio_{ijk} = \frac{P_{ij}}{P_{ik}}
$$

这里的概率可以通过共现矩阵的统计性质得到，也可以通过词向量得到，而我们的优化目标就是使得两种方法计算的概率得到的概率比率尽可能接近。我们先把两种方法得到的概率比率写出来，用共现矩阵得到的比率

$$
ratio_{ijk} = \frac{X_{ij} / X_i}{X_{ik} / X_i} = \frac{X_{ij}}{X_{ik}}
$$

用词向量得到的比率

$$
ratio_{ijk} = \frac{exp(u_j^Tv_i) / \sum\limits_l{exp(u_l^Tv_i)}}{exp(u_k^Tv_i) / \sum\limits_l{exp(u_l^Tv_i)}} = \frac{exp(u_j^Tv_i)}{exp(u_k^Tv_i)}
$$

要使得两个比率相等，最简单的是让分子分母对应相等，而在优化上，我们可以选择最小二乘法构建损失函数，即损失函数可以定义为

$$
\sum\limits_{i,j}(X_{ij} - exp(u_j^Tv_i))^2
$$

很多时候我们会取对数，也就是损失函数变为了

$$
\sum\limits_{i,j}(u_j^Tv_i - logX_{ij})^2
$$

:::details
好吧，与[论文](https://aclanthology.org/D14-1162/)少了一个系数 $X_i$ ，论文的推导是通过向量的查与概率比率相等来求的，但这不是导致差异的原因，真正的原因是原论文在概率比率上没有约去分母，而是直接保留了分母，而后令分子分母分别相等，就比我们的推导多了个 $X_i$ 。用分子分母分别相等解这个方程本身就匪夷所思，我不清楚为什么这样做，理论上你可以加上任意一个系数，原论文最后也没有用 $X_i$ 这个系数，而是定义了一个关于 $X_{ij}$ 的函数作为系数。
:::

然后我们考虑给每一个损失乘上一个权重 $f(X_{ij})$ ，定义为

$$
f(x) =
\begin{cases}
(x/x_{max})^\alpha & \text{if } x < x_{max} \\
1 & \text{otherwise }
\end{cases}
$$

论文中给出的一个取值是 $x_{max} = 100, \alpha = 3/4$ 。最终我们推导出来的损失函数就是

$$
\sum\limits_{i,j}f(X_{ij})(u_j^Tv_i - logX_{ij})^2
$$

原论文还有一些偏置项，可以加上。

### 基于skip-gram方法的优化的推导

简单的概括就是注意到朴素的skip-gram方法中很多的损失项计算有重复，因为单词 $i$ 和 $j$ 共现的次数可能不只一次，我们合并这些重复的项，显然系数就是 $X_{ij}$ ，也即损失函数变为了

$$
J(\theta) = \sum\limits_{i,j}X_{ij}log(Q_{ij})
$$

其中 $Q_{ij}$ 为我们通过 Softmax 或其他方法根据词向量计算出来的概率，注意到 $P_{ij} = X_{ij} / X_i$ ，这里 $P_{ij}$ 是实际的概率（或者说是共现矩阵统计的频率），上面的式子也可写作

$$
J(\theta) = \sum\limits_{i}X_i\sum\limits_{j}P_{ij}log(Q_{ij})
$$

其中 $P_{ij}log(Q_{ij})$ 就是交叉熵损失，这个很常见，但有个问题是计算开销很大，因为你需要归一化，作为改进，我们将这个损失替换为平方损失，于是写作

$$
J(\theta) = \sum\limits_{i}X_i\sum\limits_{j}(P_{ij} - Q_{ij})^2
$$

注意到平方损失不需要进行规范，我们也可以代入没有归一化的值，写作

$$
J(\theta) = \sum\limits_{i,j}X_i(X_{ij} - exp(u_j^Tv_i))^2
$$

然后考虑将系数 $X_i$ 替换为 $X_{ij}$ 的函数 $f(X_{ij})$ ，其中

$$
f(x) =
\begin{cases}
(x/x_{max})^\alpha & \text{if } x < x_{max} \\
1 & \text{otherwise }
\end{cases}
$$

于是得到了最终地损失函数的表达式

$$
\sum\limits_{i,j}f(X_{ij})(u_j^Tv_i - logX_{ij})^2
$$

当然，与上面一样，你可能还可以考虑加上偏置项。

## 词向量评估

对词向量的评估有内在评估方式和外部任务方式两种方式。内在评估比较常用的有类比词向量，将词向量相减，类比的词向量应该有相似的距离（比如brother – sister, man – woman, king - queen）；使用余弦相似度衡量词语之间的相似程度，并于人类的评估对比；针对多义单词的评估。外部评估通常考虑词向量可以应用于NLP的下游任务，一个例子是在命名实体识别任务中，寻找人名、机构名、地理位置名，词向量非常有帮助。