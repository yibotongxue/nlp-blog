---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---
# 词向量

这里为[cs224n](https://web.stanford.edu/class/cs224n/)第一讲的笔记，参考的中文资料为[这篇知乎文章](https://zhuanlan.zhihu.com/p/527211805)。

## WordNet

计算机处理文本词汇，一种方式就是 WordNet ，这是一个大型的英语词汇数据库，单词之间主要为同义关系，这些同义的单词会被分组为无序的同义词集合，而对于不同的同义词集合，相同词性的又有上位词和下位词的关系（表示泛化与具体）、整体与部分的关系、动词关系、形容词关系、副词关系，WordNet世纪由实际上由四个子图组成，跨词性的关系很少，大部分都是词法关系，还有一些关系表示名词是动词代表动作的某个语义角色。

具体的可以参考[这篇知乎文章](https://zhuanlan.zhihu.com/p/366370332)。

WordNet有许多弊端，是由少数专家创造的，本身带有主观性，而且不能精确的表示词的关系，以及增删词汇都不方便。

## 基于上下文的词汇表征

在传统的自然语言处理中，通常需要对文本进行离散表征，常见的方法是用独热向量，即只有一个维度为 $1$，其他维度为 $0$ 的向量，但这样有明显的弊端，向量的维度太高，每个向量都是正交的，无法表示词之间的相似关系。对于词之间的相似关系，一种解决方法是使用 `WordNet` ，另一种方法则是通过对大量数据的学习词向量本身的关系，从而建立稠密的、能表示词语关系的表征，其中比较有效的就是**基于上下文的词汇表征**。

所谓上下文，指的是一个词语在文本中出现时其附近的词语，通常由一个固定的窗口表示，而基于上下文的词汇表示就是通过海量数据中词语的上下文描述这个词语。这个很抽象，简单的理解可以是我们需要通过数据的学习求出一种表征方式，而这个基于上下文。

## Word2Vec

Word2Vec是一种通过迭代优化求得基于上下文的词汇表征的具体方法。其基本的思路就是通过随机初始化所有的词汇向量，而后通过最大化每一个词汇的上下文出现的似然概率求解词汇向量，这事实上是一个优化问题，优化的函数一般就是似然概率的负对数，优化方法一般是梯度下降。

### 似然函数和优化函数

对于每一个词汇，以其为中心建立一个固定大小的窗口，这个窗口中的每一个词汇即为其上下文，似然概率显然为

$$
P(o|c)
$$

其中 $o$ 表示上下文词汇，而 $c$ 表示中心词汇，而总的似然概率即为所有窗口所有上下文词汇的这个概率的乘积，即

$$
\prod_{j=1}^{T} \prod_{-m\le k\le m, k \ne 0}P(w_{j+k}|w_j)
$$

而优化的目标函数即为

$$
-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m\le k\le m, k \ne 0} log(P(w_{j+k}|w_j))
$$

这里多加了个平均，事实上在优化的问题没有区别，但可能可以使得具体调整参数的时候更为方便，学习率不需要依赖于数据量的大小（这个是我个人的理解，没有出处，慎重参考）。注意这里多了个负号，这个是必须的，因为我们需要的是使目标函数最小。

这个目标函数又成为代价函数或损失函数。

### 概率计算

概率 $P(o|c)$ 的计算通常使用 softmax 概率，我们需要先定义其相似程度，而后将相似程度取指数归一化，即可得到概率。相似程度通常通过词向量的点乘表示，因而概率即可表示为

$$
P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}
$$

其中 $v_c$ 表示中心词汇的词向量， $u_o$ 表示上下文词汇的次向量，而 $w$ 表示词汇空间所有词汇的任意一个词汇， $u_w$ 表示其词向量。

:::details 为什么使用点积
关于为什么使用点乘表示相似程度，我没有找到比较让人信服的答案。一个比较通常出现的说法是计算了余弦相似度，认为应该建立在都为单位向量的前提下（参考[知乎问题](https://www.zhihu.com/question/268273151)），这个解释是比较符合我们的直觉的，但事实上这个问题似乎不能保证所有向量的模长不会改变，对于此一个比较好的解释我们通常会进行l2归一化（参考[知乎文章](https://zhuanlan.zhihu.com/p/159244903)），但似乎不是所有的方法都有进行归一化。另一个解释是使用点乘要求了向量之间不仅在方向上相近，在量级上也相近，参考这个[知乎回答](https://www.zhihu.com/question/613803751)，但这个似乎不能理解。
我个人的理解是，可能只是大家发现直接使用点积相似度和使用余弦相似度没有太大的效果上的差异，而点积通常更快，所以就如此了。
:::

### 梯度计算

我们通常使用梯度下降的方法求解优化问题，这里我们需要优化的参数是所有的词向量。观察我们上面的式子，很容易发现损失函数实际上就是交叉熵损失，这个的梯度很好求解，你可以具体去推一下，[文章](https://zhuanlan.zhihu.com/p/527211805)中有较完整的推导了，但只是对于中间词汇的，对于上下文词汇也需要推导。
