---
head:
  - - link
    - rel: stylesheet
      href: https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css
---

# 神经机器翻译

这里主要是对神经机器翻译的一些理解，主要基于[cs224n](https://web.stanford.edu/class/cs224n/)第六、八讲的笔记，参考的中文资料为[文章](https://juejin.cn/post/7095181203929563173)和[文章](https://juejin.cn/post/7095946235848163364)。

## 语言模型

语言模型即根据当前的一个词语序列预测下一个词语的概率分布的模型。

### n-gram 语言模型

n-gram 即 n 个连续的单词形成的词块。我们假设一个单词出现的概率仅与前面 n-1 个单词相关，即我们预测下一个单词为 $w_{t+1}$ 的概率为

$$
P(w_{t}|w_{t-1},w_{t-2},\cdots,w_{t-n+1}) = \frac{P(w_{t},w_{t-1},w_{t-2},\cdots,w_{t-n+1})}{P(w_{t-1},w_{t-2},\cdots,w_{t-n+1})}
$$

即一个 n-gram 出现的概率除以一个 (n-1)-gram 出现的概率。

n-gram 语言模型会面临稀疏性的问题，即你需要查询的 n-gram 可能在语料库中没有出现过，此时就会导致概率为 0，如果作为分母则会导致除零错误，这时候我们可以减小 n 的值，称为后退处理；如果作为分子，则可以添加一个平滑项。 n 的增加，面临的稀疏性问题会越来越严重，通常 n 的值不能打与 5。

使用 n-gram 模型需要存储语料库中所有的 n-gram 的出现次数，当 n 增大时，模型的大小也会增大。

### 固定窗口的神经语言模型

我们也可以用神经网络来搭建语言模型，一个常见的就是固定窗口的神经语言模型。具体的方法就是对于固定大小的窗口中的所有词汇的词向量组成矩阵，通过一个带有一个隐藏层的线性神经网络来预测下一个单词的概率分布。这很好的避免了稀疏性的问题，并且不用存储所有的 n-gram ，但这样的模型使得扩大窗口大小就需要增加权重矩阵的大小，而窗口再大也常不足以很好的预测下一个单词。

### 循环神经网络(RNN)

循环神经网络定义的语言模型简单的说就是对于一个任意长的序列，其下一个单词的概率由最后一个单词及其之前的单词（包含最后一个单词）共同决定，而之前的单词（包括最后一个单词）由隐状态表示，隐状态的计算由当前的单词和上一个隐状态共同决定，即递归的方式。

具体的理解，可以参考[文章](https://juejin.cn/post/7095181203929563173)中的图片

![rnn](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be5f20abf3f54ff699d9cae5838e52ce~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

即对于输入序列的每一个单词，我们通过词嵌入得到其向量表示，同时我们将这个向量与上一个隐状态拼接起来，通过一个线性层得到当前的隐状态，对于第一个隐状态，一般由设计者自行初始化，可以全为 $0$ ，TensorFlow 和 Pytorch似乎都是这样做的，参考[知乎回答](https://www.zhihu.com/question/384725029/answer/1124300180)。对于输出的概率分布，一般采用最后一个状态通过一个线性层得到。

循环神经网络的训练可以通过梯度累加的方法，对于语料库中每一个单词预测其概率分布，并于实际的独热向量求交叉熵损失，将所有的损失求和平均即为总的损失，反向传播的时候将每一个时间步得到的参数的梯度加起来，即得参数总的梯度，这种方法称为"backpropagation through time"。实际操作的时候，我们往往不会将整个语料库遍历，而会选择一个句子（一批句子）或者一个文档作为一个序列。
